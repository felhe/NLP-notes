# Lecture 10: Data Annotation

- How do we produce labeled data?

## Expert Annotation
- experts label independently
- different labels possible: labels or rules unclear, multiple possible labels, human errors

1. give experts small set of examples
2. see where they disagree
3. discuss and refine rules
4. each expert annotates different subsets

### Annotation quality
- compute how much experts agree
- Inter-Annotator Agreement = (P(A) - P(E)) / (1 - P(E))
- P(A): probability that two annotators agree
- P(E): probability that two annotators agree by chance
- > 0.8: good agreement, > 0.9: excellent agreement

## Crowdsourcing
- use crowd workers to annotate data
- low payment
- bad data quality
- many non native speakers
- high bias
